{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch入门学习笔记--张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000e+00, 8.7712e-01, 2.1274e+23, 3.4034e-06],\n",
       "         [1.0579e+21, 5.4363e+22, 2.7301e-06, 1.3090e+22],\n",
       "         [1.0533e+21, 1.7680e-04, 3.2919e-09, 6.8620e-07]]),\n",
       " tensor([[0.1493, 0.7796, 0.3621, 0.0746],\n",
       "         [0.6983, 0.5425, 0.8951, 0.4312],\n",
       "         [0.9196, 0.3056, 0.5086, 0.9956]]),\n",
       " tensor([[ 1.8490,  0.3377, -0.2640, -1.5079],\n",
       "         [ 1.7335, -0.2648, -0.3296,  0.0636],\n",
       "         [-2.5631, -0.4883,  1.7666,  1.2985]]),\n",
       " tensor([[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]),\n",
       " tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]),\n",
       " tensor([2.3000, 4.0000]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建张量\n",
    "x = torch.empty(3,4)#空张量\n",
    "y = torch.rand(3,4,dtype=torch.float)# 随机张量，值在0，1之间,类型为float #类型有float,int,long,double等\n",
    "p = torch.randn(3,4) # 标准正态分布的随机值，均值为0，方差为1\n",
    "z = torch.zeros(3,4) # 零张量\n",
    "n = torch.ones(3,4) # 单位张量\n",
    "m = torch.tensor([2.3,4]) # 直接将数组类型变成张量类型\n",
    "x,y,p,z,n,m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5838, -0.0833, -0.6712, -0.6246],\n",
      "        [ 1.9392,  1.1987,  0.7184, -1.5299],\n",
      "        [-0.4571,  0.4022, -0.1025,  1.8049]])\n",
      "torch.Size([3, 4])\n",
      "tensor([ 0.5838,  1.9392, -0.4571])\n",
      "\n",
      "1x12 tensor([ 0.5838, -0.0833, -0.6712, -0.6246,  1.9392,  1.1987,  0.7184, -1.5299,\n",
      "        -0.4571,  0.4022, -0.1025,  1.8049])\n",
      "\n",
      "2x6 tensor([[ 0.5838, -0.0833, -0.6712, -0.6246,  1.9392,  1.1987],\n",
      "        [ 0.7184, -1.5299, -0.4571,  0.4022, -0.1025,  1.8049]])\n",
      "val: -0.41023898124694824 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "# 张量的基本操作\n",
    "t = torch.randn_like(y,dtype=torch.float)#从已有的张量中创建一个相同维度的张量\n",
    "print(t)\n",
    "print(t.size()) # 张量维度\n",
    "print(t[:,0]) # 取张量第一列，切片功能跟numpy相同\n",
    "print(\"\\n1x12\",t.view(12))#将3*4的张量reshape成一维张量\n",
    "print(\"\\n2x6\",t.view(2,6))# #将3*4的张量reshape成2*6张量\n",
    "num = torch.randn(1)\n",
    "print(\"val:\",num.item(),type(num.item())) #从one element tensor中取值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7330,  0.6962, -0.3091, -0.5500],\n",
      "        [ 2.6375,  1.7412,  1.6135, -1.0987],\n",
      "        [ 0.4625,  0.7078,  0.4061,  2.8005]])\n",
      "tensor([[ 0.7330,  0.6962, -0.3091, -0.5500],\n",
      "        [ 2.6375,  1.7412,  1.6135, -1.0987],\n",
      "        [ 0.4625,  0.7078,  0.4061,  2.8005]])\n",
      "tensor([[ 0.7330,  0.6962, -0.3091, -0.5500],\n",
      "        [ 2.6375,  1.7412,  1.6135, -1.0987],\n",
      "        [ 0.4625,  0.7078,  0.4061,  2.8005]])\n",
      "tensor([[ 0.7330,  0.6962, -0.3091, -0.5500],\n",
      "        [ 2.6375,  1.7412,  1.6135, -1.0987],\n",
      "        [ 0.4625,  0.7078,  0.4061,  2.8005]])\n"
     ]
    }
   ],
   "source": [
    "# 相同维度的张量相加（四种方法） \n",
    "print(y+t)\n",
    "print(torch.add(y,t))\n",
    "res = torch.empty(3,4)\n",
    "torch.add(y,t,out=res)\n",
    "print(res) # 将加和赋值给空张量\n",
    "t.add_(y) # 直接修改t\n",
    "print(t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.] <class 'torch.Tensor'> <class 'numpy.ndarray'>\n",
      "tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# torch.tensor->numpy.ndarray\n",
    "a = torch.ones(5) # 单元张量\n",
    "b = a.numpy() # 转化成numpy中的数组类型\n",
    "print(a,b,type(a),type(b))\n",
    "a.add_(1) # 将张量中每个元素加1\n",
    "print(a,b) # 可见b也随之加1！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64) <class 'numpy.ndarray'> <class 'torch.Tensor'>\n",
      "[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# numpy.ndarray->torch.tensor\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(a,b,type(a),type(b))\n",
    "np.add(a,1,out=a)\n",
    "print(a,b)# 可见b也随之加1！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], requires_grad=True)\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]], grad_fn=<AddBackward0>)\n",
      "None <AddBackward0 object at 0x104f0db10>\n",
      "tensor([[27., 27., 27., 27.],\n",
      "        [27., 27., 27., 27.],\n",
      "        [27., 27., 27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n",
      "<MulBackward0 object at 0x104f0db10>\n",
      "<MeanBackward0 object at 0x11af649d0>\n",
      "d(out)/dx: tensor([[1.5000, 1.5000, 1.5000, 1.5000],\n",
      "        [1.5000, 1.5000, 1.5000, 1.5000],\n",
      "        [1.5000, 1.5000, 1.5000, 1.5000]])\n"
     ]
    }
   ],
   "source": [
    "# 张量的自动微分\n",
    "x = torch.ones(3,4,requires_grad=True)\n",
    "print(x)\n",
    "y = x+2\n",
    "print(y)\n",
    "print(x.grad_fn,y.grad_fn) # 可见,y多了一个AddBackward 这里的x是属于用户自己定义的,而y属于函数产生的,所以y有grad_fn属性,而x没有.\n",
    "z = y*y*3\n",
    "out = z.mean()\n",
    "print(z,out)\n",
    "print(z.grad_fn) # MulBackward\n",
    "print(out.grad_fn)# MeanBackward\n",
    "out.backward() # 对out进行反向传播\n",
    "print(\"d(out)/dx:\",x.grad) # out关于x的梯度\n",
    "# 这里的out为标量,所以直接调用backward()函数即可,一定要注意当out为数组时,用先定义一样大小的Tensor例如grad_output执行.backgrad(grad_output)语句.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tensor.norm of tensor([-2.7014,  3.9266, -2.4432])>\n",
      "<bound method Tensor.norm of tensor([-5.4028,  7.8533, -4.8865])>\n",
      "<bound method Tensor.norm of tensor([-10.8055,  15.7066,  -9.7729])>\n",
      "<bound method Tensor.norm of tensor([-21.6111,  31.4131, -19.5458])>\n",
      "<bound method Tensor.norm of tensor([-43.2221,  62.8262, -39.0917])>\n",
      "<bound method Tensor.norm of tensor([-86.4443, 125.6524, -78.1833])>\n",
      "<bound method Tensor.norm of tensor([-172.8886,  251.3049, -156.3666])>\n",
      "<bound method Tensor.norm of tensor([-345.7772,  502.6097, -312.7333])>\n",
      "<bound method Tensor.norm of tensor([-691.5543, 1005.2194, -625.4666])>\n",
      "tensor([-691.5543, 1005.2194, -625.4666], grad_fn=<MulBackward0>) \n",
      "\n",
      "y在v处的梯度 tensor([1.0240e+02, 1.0240e+03, 1.0240e-01]) \n",
      "\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "tensor([-0.6753,  0.9817, -0.6108], requires_grad=True) tensor([-0.6753,  0.9817, -0.6108]) False\n",
      "tensor(True)\n",
      "True False\n"
     ]
    }
   ],
   "source": [
    "# 创建y=x*2^n的计算过程,结果为矢量\n",
    "x = torch.randn(3,requires_grad=True)\n",
    "y=x*2\n",
    "while y.data.norm()<1000:\n",
    "    y = y*2\n",
    "    print(y.data.norm)\n",
    "print(y,'\\n')\n",
    "# 计算v=[0.1,1.0,0.0001]处的梯度\n",
    "v=  torch.tensor([0.1,1.0,0.0001],dtype=torch.float)\n",
    "y.backward(v)\n",
    "print(\"y在v处的梯度\",x.grad,\"\\n\")\n",
    "# 关闭梯度的功能\n",
    "print(x.requires_grad)\n",
    "print((x**2).requires_grad)\n",
    "with torch.no_grad():\n",
    "    print((x**2).requires_grad)\n",
    "# 关闭梯度的第二种方法\n",
    "print(x.requires_grad)\n",
    "y = x.detach() # 关闭梯度\n",
    "print(x,y,y.requires_grad)\n",
    "print(x.eq(y).all()) # eq()比较相等\n",
    "print(x.requires_grad,y.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False],\n",
      "        [ True],\n",
      "        [ True]])\n",
      "相等的个数 tensor(2)\n",
      "size: tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]]) torch.Size([3, 1])\n",
      "column max: torch.return_types.max(\n",
      "values=tensor([1., 2., 3.]),\n",
      "indices=tensor([0, 0, 0]))\n",
      "row max: torch.return_types.max(\n",
      "values=tensor([3.]),\n",
      "indices=tensor([2]))\n"
     ]
    }
   ],
   "source": [
    "# 比较tensor相等\n",
    "outputs=torch.FloatTensor([[1],[2],[3]])\n",
    "targets=torch.FloatTensor([[0],[2],[3]])\n",
    "print(targets.eq(outputs.data))\n",
    "print(\"相等的个数\",targets.eq(outputs.data).sum()) # 统计相等的个数\n",
    "# 找出tensor最大值\n",
    "outputs=torch.FloatTensor([[1],[2],[3]])\n",
    "print(\"size:\",outputs,outputs.size()) \n",
    "print(\"column max:\",torch.max(outputs.data,1)) # 第1维度的最大值以及对应indices\n",
    "print(\"row max:\",torch.max(outputs.data,0)) # 第0维度最大值以及对应indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch入门学习笔记--神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 以LeNet5为例\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,6,5) # 输入为32x32？？，卷积核大小为5*5，卷积核种类为6\n",
    "        self.conv2 = nn.Conv2d(6,16,5) # 输入为前一层的6个特征，卷积核大小为5*5，卷积核种类为16\n",
    "        self.fc1 = nn.Linear(16*5*5,120)#输入为16*5*5，输出为120\n",
    "        self.fc2 = nn.Linear(120,84)# 输入为120维，输出为84维\n",
    "        self.fc3 = nn.Linear(84,10) # 输入为84维，输出为10\n",
    "    def forward(self,x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)),(2,2)) # 卷积—relu-2x2下采样\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),2) # 卷积-relu-2x2下采样 # 卷积核为方形时，可只写一个维度\n",
    "        x = x.view(-1,16*5*5)# flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0 torch.Size([6, 3, 5, 5])\n",
      "1 torch.Size([6])\n",
      "2 torch.Size([16, 6, 5, 5])\n",
      "3 torch.Size([16])\n",
      "4 torch.Size([120, 400])\n",
      "5 torch.Size([120])\n",
      "6 torch.Size([84, 120])\n",
      "7 torch.Size([84])\n",
      "8 torch.Size([10, 84])\n",
      "9 torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# 打印网络参数\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "for i in range(len(params)):\n",
    "    print(i,params[i].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n",
      "tensor([[-0.1136,  0.1119, -0.0774, -0.0855, -0.0064, -0.1242, -0.1182, -0.0673,\n",
      "          0.0925,  0.0184]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 随机输入一个向量，查看前向传播输出\n",
    "input = torch.randn(1,3,32,32)\n",
    "print(input.size())\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将梯度初始化\n",
    "net.zero_grad() \n",
    "# 调用backward()函数之前都要将梯度清零，因为如果梯度不清零，pytorch中会将上次计算的梯度和本次计算的梯度累加。\n",
    "# 这样逻辑的好处是，当我们的硬件限制不能使用更大的bachsize时，使用多次计算较小的bachsize的梯度平均值来代替，\n",
    "# 更方便，坏处当然是每次都要清零梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机一个梯度进行反向传播\n",
    "out.backward(torch.randn(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss func\n",
    "mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.5753,  0.0053,  1.1684, -2.1197,  0.2603, -0.7836,  0.8957,  0.9131,\n",
      "         1.7946,  1.8100])\n",
      "tensor([[ 1.5753,  0.0053,  1.1684, -2.1197,  0.2603, -0.7836,  0.8957,  0.9131,\n",
      "          1.7946,  1.8100]])\n",
      "tensor(1.7156, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 计算loss\n",
    "target = torch.randn(10)\n",
    "print(target)\n",
    "target = target.view(1,-1) # 变成列向量\n",
    "print(target)\n",
    "\n",
    "output = net(input)\n",
    "loss = mse(output,target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad\n",
      "tensor([ 0.0112, -0.0102,  0.0186,  0.0059, -0.0032, -0.0102])\n"
     ]
    }
   ],
   "source": [
    "# 将梯度初始化，计算上一步中loss的反向传播\n",
    "net.zero_grad()\n",
    "print(\"conv1.bias.grad\")\n",
    "print(net.conv1.bias.grad)\n",
    "loss.backward()\n",
    "print(\"conv1.bias.grad\")\n",
    "print(net.conv1.bias.grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新权重\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(),lr=0.01)\n",
    "# 使用优化器更新权重\n",
    "optimizer.zero_grad()\n",
    "output = net(input)\n",
    "loss = mse(output,target)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch入门学习笔记--分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练一个分类器\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms #transforms模块提供了一般的图像转换操作类\n",
    "# 将（0，1）区间的数据标准化到(-1,1)\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "# transforms.ToTensor() 把shape=(H x W x C)的像素值范围为[0, 255]的PIL.Image或者numpy.ndarray转换成shape=(C x H x W)的像素值范围为[0.0, 1.0]的torch.FloatTensor。\n",
    "# transforms.Normalize 此转换类作用于torch.*Tensor。给定均值(R, G, B)和标准差(R, G, B)，用公式channel = (channel - mean) / std进行规范化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "batch_size = 32\n",
    "trainset = torchvision.datasets.CIFAR10(root='/Users/yoloc/data/', train=True,download=False,transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='/Users/yoloc/data/', train=False,download=False,transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset,batch_size=batch_size,shuffle=True,num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset,batch_size=batch_size,shuffle=True,num_workers=2)\n",
    "classes = ('plane','car','bird','cat','deer','dog','frog','horse','ship','truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2=Net() # 使用前面的lenet网络\n",
    "cet = nn.CrossEntropyLoss()\n",
    "optimizer2= optim.SGD(net2.parameters(),lr=0.001,momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  100] loss: 0.100\n",
      "[1,  200] loss: 0.100\n",
      "[1,  300] loss: 0.099\n",
      "[1,  400] loss: 0.097\n",
      "[1,  500] loss: 0.097\n",
      "[1,  600] loss: 0.096\n",
      "[1,  700] loss: 0.094\n",
      "[1,  800] loss: 0.094\n",
      "[1,  900] loss: 0.091\n",
      "[1, 1000] loss: 0.092\n",
      "[1, 1100] loss: 0.089\n",
      "[1, 1200] loss: 0.088\n",
      "[1, 1300] loss: 0.086\n",
      "[1, 1400] loss: 0.088\n",
      "[1, 1500] loss: 0.087\n",
      "[2,  100] loss: 0.084\n",
      "[2,  200] loss: 0.085\n",
      "[2,  300] loss: 0.084\n",
      "[2,  400] loss: 0.081\n",
      "[2,  500] loss: 0.080\n",
      "[2,  600] loss: 0.081\n",
      "[2,  700] loss: 0.081\n",
      "[2,  800] loss: 0.080\n",
      "[2,  900] loss: 0.080\n",
      "[2, 1000] loss: 0.079\n",
      "[2, 1100] loss: 0.078\n",
      "[2, 1200] loss: 0.076\n",
      "[2, 1300] loss: 0.079\n",
      "[2, 1400] loss: 0.077\n",
      "[2, 1500] loss: 0.077\n",
      "[3,  100] loss: 0.075\n",
      "[3,  200] loss: 0.075\n",
      "[3,  300] loss: 0.076\n",
      "[3,  400] loss: 0.076\n",
      "[3,  500] loss: 0.073\n",
      "[3,  600] loss: 0.073\n",
      "[3,  700] loss: 0.072\n",
      "[3,  800] loss: 0.075\n",
      "[3,  900] loss: 0.073\n",
      "[3, 1000] loss: 0.074\n",
      "[3, 1100] loss: 0.074\n",
      "[3, 1200] loss: 0.074\n",
      "[3, 1300] loss: 0.072\n",
      "[3, 1400] loss: 0.072\n",
      "[3, 1500] loss: 0.072\n",
      "[4,  100] loss: 0.070\n",
      "[4,  200] loss: 0.070\n",
      "[4,  300] loss: 0.070\n",
      "[4,  400] loss: 0.071\n",
      "[4,  500] loss: 0.069\n",
      "[4,  600] loss: 0.069\n",
      "[4,  700] loss: 0.071\n",
      "[4,  800] loss: 0.071\n",
      "[4,  900] loss: 0.068\n",
      "[4, 1000] loss: 0.069\n",
      "[4, 1100] loss: 0.068\n",
      "[4, 1200] loss: 0.069\n",
      "[4, 1300] loss: 0.068\n",
      "[4, 1400] loss: 0.068\n",
      "[4, 1500] loss: 0.068\n",
      "[5,  100] loss: 0.066\n",
      "[5,  200] loss: 0.066\n",
      "[5,  300] loss: 0.067\n",
      "[5,  400] loss: 0.066\n",
      "[5,  500] loss: 0.064\n",
      "[5,  600] loss: 0.066\n",
      "[5,  700] loss: 0.067\n",
      "[5,  800] loss: 0.065\n",
      "[5,  900] loss: 0.067\n",
      "[5, 1000] loss: 0.067\n",
      "[5, 1100] loss: 0.064\n",
      "[5, 1200] loss: 0.066\n",
      "[5, 1300] loss: 0.066\n",
      "[5, 1400] loss: 0.067\n",
      "[5, 1500] loss: 0.064\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    for i,data in enumerate(trainloader,0):\n",
    "        inputs,labels=data\n",
    "        optimizer2.zero_grad()\n",
    "        outputs = net2(inputs) # 前馈\n",
    "        loss = cet(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "        running_loss+=loss.item()\n",
    "        if i%100==99:\n",
    "            print('[%d,%5d] loss: %.3f' % (epoch+1,i+1,running_loss/2000))\n",
    "            running_loss=0.0\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:   car plane  ship truck\n"
     ]
    }
   ],
   "source": [
    "# 使用模型预测\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "dataiter = iter(testloader)\n",
    "images,labels = dataiter.next()\n",
    "# plt.imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth:',' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:   car  bird plane truck\n"
     ]
    }
   ],
   "source": [
    "outputs = net2(images)\n",
    "_,predicted = torch.max(outputs,1)\n",
    "print('Predicted:',' '.join('%5s' % classes[predicted[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5232 10000\n",
      "acc on the 10000 test images: 52 %\n"
     ]
    }
   ],
   "source": [
    "# 打分\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images,labels = data\n",
    "        outputs = net2(images)\n",
    "        _,predicted = torch.max(outputs.data,1)\n",
    "        total+=labels.size(0)\n",
    "        correct += (predicted==labels).sum().item()\n",
    "print(correct,total)\n",
    "print('acc on the %d test images: %d %%' % (total,100 * correct /total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 存取模型\n",
    "PATH='./cifar_net.pth'\n",
    "torch.save(net.state_dict(),PATH) # 保存\n",
    "pretrained_net = torch.load(PATH) # 读取\n",
    "net3 = Net()\n",
    "net3.load_state_dict(pretrained_net) # 加载权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
